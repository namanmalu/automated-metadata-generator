{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02d3314c",
   "metadata": {},
   "source": [
    "#  Automated Metadata Generator \n",
    "\n",
    "This notebook walks through the complete project for extracting smart metadata from `.docx`, `.pdf`, and `.txt` files using Python and NLP.\n",
    "\n",
    "The project consists of:\n",
    "- A **Streamlit app** to upload files and view metadata interactively\n",
    "- A **backend module** with functions to extract and structure metadata\n",
    "- **OCR** support for scanned documents\n",
    "- Lightweight **NLP** using spaCy and TF-IDF\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658b78ea",
   "metadata": {},
   "source": [
    "##  Backend: `metadata_utils.py`\n",
    "This file handles the core functionality: extracting text from documents, identifying named entities, important sections, and producing structured metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f011700b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import docx\n",
    "import fitz  # PyMuPDF\n",
    "import pytesseract\n",
    "\n",
    "from collections import Counter\n",
    "import spacy\n",
    "from en_core_web_sm import load as load_model\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "nlp = load_model()\n",
    "\n",
    "SECTION_HEADINGS = [\n",
    "    \"abstract\", \"introduction\", \"objective\", \"problem\",\n",
    "    \"conclusion\", \"summary\", \"results\", \"discussion\"\n",
    "]\n",
    "\n",
    "def get_named_entities(text):\n",
    "    doc = nlp(text)\n",
    "    entities = {\"people\": [], \"organizations\": [], \"locations\": [], \"dates\": []}\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"PERSON\":\n",
    "            entities[\"people\"].append(ent.text)\n",
    "        elif ent.label_ == \"ORG\":\n",
    "            entities[\"organizations\"].append(ent.text)\n",
    "        elif ent.label_ == \"GPE\":\n",
    "            entities[\"locations\"].append(ent.text)\n",
    "        elif ent.label_ == \"DATE\":\n",
    "            entities[\"dates\"].append(ent.text)\n",
    "    for key in entities:\n",
    "        entities[key] = list(set(entities[key]))\n",
    "    return entities\n",
    "\n",
    "def extract_sections(text):\n",
    "    lines = text.split(\"\\n\")\n",
    "    current_section = None\n",
    "    sections = {}\n",
    "    for line in lines:\n",
    "        clean_line = line.strip()\n",
    "        line_lower = clean_line.lower()\n",
    "        if any(heading in line_lower for heading in SECTION_HEADINGS):\n",
    "            clean_heading = re.sub(r\"^[0-9]+[.)]?\\s*\", \"\", clean_line)\n",
    "            current_section = clean_heading\n",
    "            sections[current_section] = \"\"\n",
    "        elif current_section:\n",
    "            sections[current_section] += clean_line + \" \"\n",
    "    return sections\n",
    "\n",
    "def get_top_sentences(text, n=5):\n",
    "    sentences = re.split(r'(?<=[.!?]) +', text)\n",
    "    if len(sentences) <= n:\n",
    "        return sentences\n",
    "    tfidf = TfidfVectorizer(stop_words='english')\n",
    "    tfidf_matrix = tfidf.fit_transform(sentences)\n",
    "    sim_scores = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix).flatten()\n",
    "    top_indices = sim_scores.argsort()[-n:][::-1]\n",
    "    return [sentences[i] for i in top_indices if i < len(sentences)]\n",
    "\n",
    "def structure_metadata(metadata_dict):\n",
    "    structured = {}\n",
    "    for key, value in metadata_dict.items():\n",
    "        if isinstance(value, list):\n",
    "            structured[key] = [str(v).strip() for v in value if str(v).strip()]\n",
    "        elif isinstance(value, dict):\n",
    "            structured[key] = {k: list(set(map(str, v))) for k, v in value.items() if isinstance(v, list)}\n",
    "        elif value not in [None, \"\", \"None\"]:\n",
    "            structured[key] = str(value).strip()\n",
    "    return structured\n",
    "\n",
    "def extract_smart_metadata(text, filename=\"\"):\n",
    "    words = re.findall(r'\\b\\w{4,}\\b', text.lower())\n",
    "    common_words = Counter(words).most_common(10)\n",
    "    top_sentences = get_top_sentences(text)\n",
    "    entities = get_named_entities(text)\n",
    "    sections = extract_sections(text)\n",
    "\n",
    "    summary = sections.get(\"summary\", \"\") or sections.get(\"conclusion\", \"\") or \" \".join(top_sentences[:2])\n",
    "    objective = sections.get(\"objective\", \"\") or sections.get(\"introduction\", \"\") or \" \".join(top_sentences[2:4])\n",
    "\n",
    "    metadata = {\n",
    "        \"filename\": filename,\n",
    "        \"word_count\": len(words),\n",
    "        \"character_count\": len(text),\n",
    "        \"top_keywords\": [word for word, _ in common_words],\n",
    "        \"key_sentences\": top_sentences,\n",
    "        \"summary\": summary.strip(),\n",
    "        \"purpose\": objective.strip(),\n",
    "        \"named_entities\": entities,\n",
    "        \"sections_found\": list(sections.keys())\n",
    "    }\n",
    "\n",
    "    return structure_metadata(metadata)\n",
    "\n",
    "def extract_docx_metadata(docx_path):\n",
    "    doc = docx.Document(docx_path)\n",
    "    full_text = \"\\n\".join([para.text for para in doc.paragraphs])\n",
    "    title = doc.paragraphs[0].text.strip() if doc.paragraphs else \"Untitled\"\n",
    "    metadata = extract_smart_metadata(full_text, os.path.basename(docx_path))\n",
    "    metadata[\"title\"] = title\n",
    "    return structure_metadata(metadata)\n",
    "\n",
    "def extract_pdf_metadata(pdf_path):\n",
    "    from pdf2image import convert_from_path\n",
    "    text = \"\"\n",
    "    try:\n",
    "        with fitz.open(pdf_path) as doc:\n",
    "            for page in doc:\n",
    "                page_text = page.get_text()\n",
    "                if page_text.strip():\n",
    "                    text += page_text\n",
    "        if not text.strip():\n",
    "            images = convert_from_path(pdf_path)\n",
    "            for img in images:\n",
    "                text += pytesseract.image_to_string(img)\n",
    "    except Exception as e:\n",
    "        text = f\"Error reading PDF: {str(e)}\"\n",
    "    return structure_metadata(extract_smart_metadata(text, os.path.basename(pdf_path)))\n",
    "\n",
    "def extract_txt_metadata(txt_path):\n",
    "    with open(txt_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    return structure_metadata(extract_smart_metadata(text, os.path.basename(txt_path)))\n",
    "\n",
    "def extract_metadata(file_path):\n",
    "    ext = os.path.splitext(file_path)[1].lower()\n",
    "    if ext == \".pdf\":\n",
    "        return extract_pdf_metadata(file_path)\n",
    "    elif ext == \".docx\":\n",
    "        return extract_docx_metadata(file_path)\n",
    "    elif ext == \".txt\":\n",
    "        return extract_txt_metadata(file_path)\n",
    "    else:\n",
    "        return structure_metadata({\n",
    "            \"error\": \"Unsupported file type\",\n",
    "            \"filename\": os.path.basename(file_path)\n",
    "        })\n",
    "\n",
    "def convert_metadata_to_csv(metadata):\n",
    "    from io import StringIO\n",
    "    import csv\n",
    "\n",
    "    output = StringIO()\n",
    "    writer = csv.writer(output)\n",
    "    writer.writerow([\"Field\", \"Value\"])\n",
    "    for key, value in metadata.items():\n",
    "        if isinstance(value, list):\n",
    "            value = \", \".join(map(str, value))\n",
    "        elif isinstance(value, dict):\n",
    "            value = \", \".join([f\"{k}: {', '.join(v)}\" for k, v in value.items()])\n",
    "        writer.writerow([key, value])\n",
    "    return output.getvalue()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def154f7",
   "metadata": {},
   "source": [
    "## üñº Frontend: `app.py`\n",
    "The Streamlit-based UI for the system. It lets users upload documents, shows extracted metadata, and allows downloading the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f77f0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import os\n",
    "import json\n",
    "from metadata_utils import extract_metadata\n",
    "\n",
    "st.set_page_config(\n",
    "    page_title=\"Automated Metadata Generator\",\n",
    "    page_icon=\"üìÑ\",\n",
    "    layout=\"wide\",\n",
    "    initial_sidebar_state=\"expanded\"\n",
    ")\n",
    "\n",
    "# --- Custom CSS ---\n",
    "st.markdown(\"\"\"\n",
    "    <style>\n",
    "    body, .stApp {\n",
    "        background: linear-gradient(120deg, #18181c 0%, #23243a 100%) !important;\n",
    "        color: #fff !important;\n",
    "    }\n",
    "    .main-banner {\n",
    "        display: flex;\n",
    "        align-items: center;\n",
    "        justify-content: center;\n",
    "        background: linear-gradient(90deg, #141e30 0%, #243b55 100%);\n",
    "        border-radius: 24px;\n",
    "        padding: 40px 32px;\n",
    "        margin-bottom: 32px;\n",
    "        box-shadow: 0 8px 40px 0 rgba(0,0,0,0.45);\n",
    "    }\n",
    "    .banner-title {\n",
    "        font-size: 3em;\n",
    "        font-weight: bold;\n",
    "        color: #fff;\n",
    "        margin-bottom: 0.2em;\n",
    "        letter-spacing: 2px;\n",
    "        text-shadow: 0 4px 32px #0008;\n",
    "    }\n",
    "    .banner-subtitle {\n",
    "        font-size: 1.25em;\n",
    "        color: #e0e7ef;\n",
    "        margin-bottom: 0;\n",
    "        font-weight: 400;\n",
    "    }\n",
    "    .feature-container {\n",
    "        display: flex;\n",
    "        overflow-x: auto;\n",
    "        gap: 24px;\n",
    "        padding-bottom: 12px;\n",
    "        margin-bottom: 36px;\n",
    "    }\n",
    "    .feature-card {\n",
    "        flex: 0 0 auto;\n",
    "        background: linear-gradient(135deg, #23243a 60%, #3e206d 100%);\n",
    "        border-radius: 18px;\n",
    "        box-shadow: 0 4px 32px 0 rgba(72,0,128,0.25);\n",
    "        padding: 24px 26px 20px 26px;\n",
    "        min-width: 280px;\n",
    "        color: #fff;\n",
    "        border: 1.5px solid #4B8BBE33;\n",
    "        transition: transform 0.18s;\n",
    "    }\n",
    "    .feature-card:hover {\n",
    "        transform: scale(1.04) translateY(-6px);\n",
    "        box-shadow: 0 8px 48px 0 rgba(72,0,128,0.38);\n",
    "        border: 1.5px solid #4B8BBE;\n",
    "    }\n",
    "    .feature-title {\n",
    "        font-size: 1.15em;\n",
    "        font-weight: 600;\n",
    "        margin-bottom: 0.5em;\n",
    "        color: #ff3c78;\n",
    "        letter-spacing: 1px;\n",
    "    }\n",
    "    .feature-desc {\n",
    "        font-size: 1em;\n",
    "        color: #e0e7ef;\n",
    "        font-weight: 400;\n",
    "    }\n",
    "    </style>\n",
    "\"\"\", unsafe_allow_html=True)\n",
    "\n",
    "# --- Main Banner ---\n",
    "st.markdown(\"\"\"\n",
    "    <div class=\"main-banner\">\n",
    "        <div>\n",
    "            <div class=\"banner-title\">üìÑ Automated Metadata Generator</div>\n",
    "            <div class=\"banner-subtitle\">\n",
    "                AI-powered, automatic, and beautifully simple ‚Äî for DOCX, PDF, and TXT files.\n",
    "            </div>\n",
    "        </div>\n",
    "    </div>\n",
    "\"\"\", unsafe_allow_html=True)\n",
    "\n",
    "# --- Features Section ---\n",
    "st.markdown('<div class=\"feature-container\">', unsafe_allow_html=True)\n",
    "features = [\n",
    "    {\"title\": \"Automating Metadata Generation\", \"desc\": \"Auto-generates metadata for diverse documents.\"},\n",
    "    {\"title\": \"Content Extraction\", \"desc\": \"Extracts text from PDF, DOCX, TXT using OCR where needed.\"},\n",
    "    {\"title\": \"Semantic Content Identification\", \"desc\": \"Leverages key sections of documents intelligently.\"},\n",
    "    {\"title\": \"Structured Metadata Creation\", \"desc\": \"Outputs clean, structured, machine-readable metadata.\"},\n",
    "    {\"title\": \"Easy-to-Use Interface\", \"desc\": \"Simple web app with beautiful design and usability.\"},\n",
    "    {\"title\": \"Supports Multiple Formats\", \"desc\": \"Works with .docx, .pdf, and .txt files.\"},\n",
    "]\n",
    "for f in features:\n",
    "    st.markdown(f\"\"\"\n",
    "        <div class=\"feature-card\">\n",
    "            <div class=\"feature-title\">{f['title']}</div>\n",
    "            <div class=\"feature-desc\">{f['desc']}</div>\n",
    "        </div>\n",
    "    \"\"\", unsafe_allow_html=True)\n",
    "st.markdown('</div>', unsafe_allow_html=True)\n",
    "\n",
    "# --- Upload + Metadata Display ---\n",
    "st.markdown(\"<h3 style='color:#ff3c78;'>üìÇ Upload Your Document (DOCX / PDF / TXT)</h3>\", unsafe_allow_html=True)\n",
    "file = st.file_uploader(\"Drag or click to upload a file\", type=[\"docx\", \"pdf\", \"txt\"])\n",
    "\n",
    "if file:\n",
    "    file_ext = file.name.split(\".\")[-1].lower()\n",
    "    temp_path = f\"temp_uploaded.{file_ext}\"\n",
    "    with open(temp_path, \"wb\") as f:\n",
    "        f.write(file.read())\n",
    "\n",
    "    with st.spinner(\"üîç Extracting metadata...\"):\n",
    "        metadata = extract_metadata(temp_path)\n",
    "        if \"filename\" in metadata:\n",
    "            metadata[\"filename\"] = file.name\n",
    "        metadata = {k: v for k, v in metadata.items() if v and v != [] and v != {} and v != \"None\"}\n",
    "\n",
    "    if \"error\" in metadata:\n",
    "        st.error(metadata[\"error\"])\n",
    "    else:\n",
    "        st.markdown(\"\"\"\n",
    "            <div class=\"sexy-card\">\n",
    "                <h4 style='color:#4B8BBE;'>‚úÖ Extracted Metadata:</h4>\n",
    "        \"\"\", unsafe_allow_html=True)\n",
    "        st.json(metadata)\n",
    "        st.markdown(\"</div>\", unsafe_allow_html=True)\n",
    "\n",
    "        st.download_button(\n",
    "            \"‚¨áÔ∏è Download Metadata (JSON)\",\n",
    "            json.dumps(metadata, indent=4),\n",
    "            file_name=\"metadata.json\",\n",
    "            mime=\"application/json\"\n",
    "        )\n",
    "\n",
    "    os.remove(temp_path)\n",
    "else:\n",
    "    st.info(\"Please upload a DOCX, PDF, or TXT file to get started.\")\n",
    "\n",
    "# --- Footer ---\n",
    "st.markdown(\"\"\"\n",
    "    <hr style=\"margin-top: 3em; margin-bottom: 1em; border: 1px solid #333;\">\n",
    "    <div style=\"text-align: center; color: #888; font-size: 0.9em;\">\n",
    "        Made with ‚ù§Ô∏è using Streamlit & Python NLP ¬∑ 2025\n",
    "    </div>\n",
    "\"\"\", unsafe_allow_html=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b719eb05",
   "metadata": {},
   "source": [
    "##  NLP Bootstrap: `__init__.py`\n",
    "To ensure compatibility and faster cold-starts, this initializes a minimal spaCy model using `spacy.blank('en')`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4b81fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load():\n",
    "    import spacy\n",
    "    return spacy.blank(\"en\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a70a31",
   "metadata": {},
   "source": [
    "## Model Metadata: `meta.json`\n",
    "A description file for the lightweight NLP pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523b2684",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = {\n",
    "    \"lang\": \"en\",\n",
    "    \"name\": \"core_web_sm\",\n",
    "    \"pipeline\": [\n",
    "        \"tok2vec\",\n",
    "        \"tagger\",\n",
    "        \"parser\",\n",
    "        \"ner\"\n",
    "    ],\n",
    "    \"version\": \"3.5.0\"\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
